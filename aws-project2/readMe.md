# ğŸš€ AWS Data Engineering ETL Pipeline (S3 + Glue + PySpark)

## ğŸ“Œ Project Overview
This project demonstrates a complete **data engineering ETL pipeline** built using **AWS Glue** and **PySpark**.  
It reads raw CSV data from **Amazon S3**, cleans and transforms it in **AWS Glue**, and writes the processed data back to **S3**.

This pipeline automates data cleaning and prepares data for analytics or further processing.

---

## ğŸ§  Architecture
S3 (Raw Data)
â†“
AWS Glue Crawler â†’ AWS Glue Data Catalog (Schema)
â†“
AWS Glue ETL Job (PySpark Script)
â†“
S3 (Cleaned Data)

---


---

## âš™ï¸ Tools & Services Used
- **AWS S3** â€“ Source & destination data storage
- **AWS Glue Crawler** â€“ Automatically detect schema from CSV
- **AWS Glue Data Catalog** â€“ Stores table metadata
- **AWS Glue Job (PySpark)** â€“ Transform and clean data
- **AWS IAM** â€“ Role permissions for Glue to access S3
- **PySpark** â€“ Transformation logic (drop nulls, clean data)

---

## ğŸ§° Steps Implemented

1. **Uploaded Raw Data**  
   - `sales_data.csv` uploaded to S3 bucket: `rohaan-etl-input`

2. **Created Glue Crawler**  
   - Crawled the S3 path  
   - Created database: `etl_db`  
   - Created table: `rohaan_etl_input`

3. **Created Glue ETL Job**  
   - Type: Spark Script (Python)  
   - Script reads from Glue Catalog and cleans data

4. **ETL Script Logic:**
   ```python
   datasource = glueContext.create_dynamic_frame.from_catalog(
       database = "etl_db",
       table_name = "rohaan_etl_input"
   )

   cleaned_data = datasource.drop_null_fields()

   glueContext.write_dynamic_frame.from_options(
       frame = cleaned_data,
       connection_type = "s3",
       connection_options = {"path": "s3://rohaan-etl-output/cleaned_data/"},
       format = "csv"
   )
  
Fixed Permissions

Attached AmazonS3FullAccess to IAM role AWSGlueServiceRole-rohaan

Ran ETL Job Successfully

Output written to s3://rohaan-etl-output/cleaned_data/

Verified Output

Downloaded and viewed cleaned CSV (no nulls)

ğŸ“Š Results

âœ… Cleaned dataset stored in S3
âœ… Null values removed
âœ… Schema inferred automatically via Glue Catalog

ğŸ§  Key Skills Demonstrated

-Data Engineering (ETL)
-AWS Glue (PySpark)
-S3 Data Lake Architecture
-IAM Role Configuration
-Data Cleaning
-Cloud Automation

